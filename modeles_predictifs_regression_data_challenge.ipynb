{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "218a2b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "from sklearn.model_selection import KFold, RandomizedSearchCV\n",
    "from sklearn.metrics import r2_score, mean_squared_error, make_scorer\n",
    "import joblib\n",
    "from scipy.stats import uniform, randint\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.inspection import permutation_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de2a25b",
   "metadata": {},
   "source": [
    "#  Régression Linéaire avec ACP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4036d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Nombre de composantes retenues : 7\n",
      "Dimensions : train = (85500, 126), test = (28500, 126)\n",
      " R² sur le jeu de validation : 0.2614\n",
      "Fichier 'submission_linear_pca.csv' créé avec succès !\n"
     ]
    }
   ],
   "source": [
    "# --- Chargement des jeux d'entraînement et de test ---\n",
    "train = pd.read_csv(\"train_data_with_genres.csv\", sep=\";\", encoding=\"utf-8\", low_memory=False)\n",
    "test = pd.read_csv(\"test_data_with_genres.csv\", sep=\";\", encoding=\"utf-8\", low_memory=False)\n",
    "\n",
    "# --- Normalisation des décimales : remplace ',' par '.' (format EU -> US) ---\n",
    "train = train.replace(\",\", \".\", regex=True)\n",
    "test = test.replace(\",\", \".\", regex=True)\n",
    "\n",
    "# --- Séparation de la cible et suppression de 'popularity' des features ---\n",
    "y = pd.to_numeric(train[\"popularity\"], errors=\"coerce\").fillna(0)\n",
    "train = train.drop(columns=[\"popularity\"], errors=\"ignore\")\n",
    "\n",
    "# --- Sauvegarde des identifiants test et retrait de 'row_id' des features ---\n",
    "test_ids = test[\"row_id\"]\n",
    "train = train.drop(columns=[\"row_id\"], errors=\"ignore\")\n",
    "test = test.drop(columns=[\"row_id\"], errors=\"ignore\")\n",
    "\n",
    "# --- Liste des variables audio pour l'ACP ---\n",
    "audio_features = [\n",
    "    \"danceability\", \"energy\", \"loudness\", \"speechiness\", \"acousticness\",\n",
    "    \"instrumentalness\", \"liveness\", \"valence\", \"tempo\"\n",
    "]\n",
    "\n",
    "# --- Séparation des blocs : audio (pour ACP) vs autres variables (métadonnées, one-hot, etc.) ---\n",
    "X_audio = train[audio_features].apply(pd.to_numeric, errors=\"coerce\").fillna(0)\n",
    "X_other = train.drop(columns=audio_features).apply(pd.to_numeric, errors=\"coerce\").fillna(0)\n",
    "\n",
    "test_audio = test[audio_features].apply(pd.to_numeric, errors=\"coerce\").fillna(0)\n",
    "test_other = test.drop(columns=audio_features).apply(pd.to_numeric, errors=\"coerce\").fillna(0)\n",
    "\n",
    "# --- Sécurisation : aligne les colonnes non-audio du test sur celles du train ---\n",
    "missing_cols = set(X_other.columns) - set(test_other.columns)\n",
    "for c in missing_cols:\n",
    "    test_other[c] = 0\n",
    "test_other = test_other[X_other.columns]\n",
    "\n",
    "# --- Standardisation des features audio (centrage-réduction) avant ACP ---\n",
    "scaler = StandardScaler()\n",
    "X_audio_scaled = scaler.fit_transform(X_audio)\n",
    "test_audio_scaled = scaler.transform(test_audio)\n",
    "\n",
    "# --- ACP : conserve 90% de la variance sur le bloc audio ---\n",
    "pca = PCA(n_components=0.90, svd_solver='full')\n",
    "X_pca = pca.fit_transform(X_audio_scaled)\n",
    "test_pca = pca.transform(test_audio_scaled)\n",
    "\n",
    "print(f\" Nombre de composantes retenues : {pca.n_components_}\")\n",
    "\n",
    "# --- Fusion des composantes principales (audio) avec les autres variables ---\n",
    "X_final = np.concatenate([X_pca, X_other.values], axis=1)\n",
    "test_final = np.concatenate([test_pca, test_other.values], axis=1)\n",
    "\n",
    "print(f\"Dimensions : train = {X_final.shape}, test = {test_final.shape}\")\n",
    "\n",
    "# --- Split apprentissage/validation (80/20) ---\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_final, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# --- Modèle de Régression Linéaire (moindres carrés ordinaires) ---\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "# --- Évaluation sur le set de validation ---\n",
    "y_pred = reg.predict(X_val)\n",
    "r2_val = r2_score(y_val, y_pred)\n",
    "print(f\" R² sur le jeu de validation : {r2_val:.4f}\")\n",
    "\n",
    "# --- Prédictions sur le jeu de test et création du fichier de soumission ---\n",
    "preds = reg.predict(test_final)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"row_id\": test_ids,\n",
    "    \"popularity\": preds\n",
    "})\n",
    "\n",
    "submission.to_csv(\"submission_linear_pca.csv\", index=False, sep=\",\", encoding=\"utf-8\", float_format=\"%.6f\")\n",
    "print(\"Fichier 'submission_linear_pca.csv' créé avec succès !\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4574494f",
   "metadata": {},
   "source": [
    "#  Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ddfd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "#  Fonction utilitaire : conversion robuste des données en numérique\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "def to_numeric_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    for c in out.columns:\n",
    "        out[c] = pd.to_numeric(out[c].astype(str).str.replace(',', '.', regex=False), errors='coerce')\n",
    "    return out.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd31b749",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------------------------------------\n",
    "#  Chargement des données\n",
    "# ------------------------------------------------------------\n",
    "train = pd.read_csv(\"train_data_with_genres.csv\", sep=\";\", encoding=\"utf-8\", low_memory=False)\n",
    "test = pd.read_csv(\"test_data_with_genres.csv\", sep=\";\", encoding=\"utf-8\", low_memory=False)\n",
    "\n",
    "# Extraction de la variable cible\n",
    "\n",
    "y = train[\"popularity\"].values\n",
    "\n",
    "# Sauvegarde des row_id pour la soumission finale\n",
    "\n",
    "test_row_id = test[\"row_id\"].values if \"row_id\" in test.columns else np.arange(len(test))\n",
    "\n",
    "# Suppression des colonnes non nécessaires pour l'entraînement\n",
    "\n",
    "drop_cols = [c for c in [\"row_id\", \"popularity\"] if c in train.columns]\n",
    "train = train.drop(columns=drop_cols, errors=\"ignore\")\n",
    "test = test.drop(columns=[\"row_id\"], errors=\"ignore\")\n",
    "\n",
    "# Conversion en numérique (corrige \"0,754\", \"5,00E-05\", etc.)\n",
    "\n",
    "train = to_numeric_df(train)\n",
    "test = to_numeric_df(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f0a14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "#  PCA (Analyse en Composantes Principales) sur les variables audio\n",
    "# ------------------------------------------------------------\n",
    "# Liste des colonnes audio sur lesquelles appliquer la PCA\n",
    "\n",
    "audio_features = [\n",
    "    \"danceability\", \"energy\", \"loudness\", \"speechiness\", \"acousticness\",\n",
    "    \"instrumentalness\", \"liveness\", \"valence\", \"tempo\"\n",
    "]\n",
    "# On garde seulement celles présentes dans les données (sécurité)\n",
    "\n",
    "audio_features = [f for f in audio_features if f in train.columns]\n",
    "# Application de la PCA : 90 % de variance expliquée\n",
    "\n",
    "pca = PCA(n_components=0.90, svd_solver=\"full\", random_state=42)\n",
    "X_audio_train = train[audio_features].values\n",
    "X_audio_test = test[audio_features].values\n",
    "\n",
    "# Transformation PCA\n",
    "\n",
    "X_pca_train = pca.fit_transform(X_audio_train)\n",
    "\n",
    "X_pca_test = pca.transform(X_audio_test)\n",
    "\n",
    "# Création de DataFrames pour les composantes principales\n",
    "\n",
    "pc_cols = [f\"PC{i+1}\" for i in range(X_pca_train.shape[1])]\n",
    "df_pca_train = pd.DataFrame(X_pca_train, columns=pc_cols, index=train.index)\n",
    "df_pca_test = pd.DataFrame(X_pca_test, columns=pc_cols, index=test.index)\n",
    "# On conserve aussi les autres features (non audio)\n",
    "\n",
    "X_base_train = train.drop(columns=audio_features, errors=\"ignore\")\n",
    "X_base_test = test.drop(columns=audio_features, errors=\"ignore\")\n",
    "# Fusion des features PCA + autres\n",
    "\n",
    "X_full_train = pd.concat([df_pca_train, X_base_train], axis=1)\n",
    "X_full_test = pd.concat([df_pca_test, X_base_test], axis=1)\n",
    "# Alignement des colonnes train/test (important avant prédiction)\n",
    "\n",
    "X_full_test = X_full_test.reindex(columns=X_full_train.columns, fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f3d833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "#  Division du jeu de données (80% train / 20% validation)\n",
    "# ------------------------------------------------------------\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_full_train, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "#  Optimisation des hyperparamètres\n",
    "# ------------------------------------------------------------\n",
    "param_distributions = {\n",
    "    'n_estimators': [300,400,500, 600, 700, 800, 900, 1000],\n",
    "    'max_features': [0.3, 0.4, 0.5, 0.6 ,0.7,0.8, 'sqrt', 'log2'],\n",
    "    'max_depth': [None, 20, 30, 40, 50, 60,70,80],  \n",
    "    'min_samples_split': [2,3,4, 5, 10, 15],\n",
    "    'min_samples_leaf': [1, 2, 4, 6],\n",
    "    'bootstrap': [True]\n",
    "}\n",
    "\n",
    "# Modèle de base avec score OOB activé (Out-of-Bag)\n",
    "\n",
    "rf_base = RandomForestRegressor(oob_score=True, n_jobs=-1, random_state=42)\n",
    "\n",
    "# --- Étape 1 : RandomizedSearchCV (exploration large de l’espace) ---\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=rf_base,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=30,\n",
    "    cv=3,\n",
    "    scoring='r2',\n",
    "    n_jobs=-1,\n",
    "    verbose=0,\n",
    "    random_state=42\n",
    ")\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = random_search.best_params_\n",
    "\n",
    "# --- Étape 2 : GridSearchCV (affinage local autour des meilleurs params) ---\n",
    "\n",
    "def create_neighborhood(value, param_name):\n",
    "    if param_name == 'n_estimators':\n",
    "        return [max(100, value - 100), value, value + 100]\n",
    "    elif param_name == 'max_depth':\n",
    "        return [max(10, value - 10), value, value + 10, None] if value else [None, 40, 50, 60]\n",
    "    elif param_name == 'min_samples_split':\n",
    "        return [max(2, value - 2), value, value + 2]\n",
    "    elif param_name == 'min_samples_leaf':\n",
    "        return [max(1, value - 1), value, value + 1]\n",
    "    elif param_name == 'max_features':\n",
    "        return [value] if isinstance(value, str) else [max(0.2, value - 0.1), value, min(1.0, value + 0.1)]\n",
    "    return [value]\n",
    "\n",
    "param_grid = {p: create_neighborhood(v, p) for p, v in best_params.items()}\n",
    "\n",
    "# Lancement du GridSearch (plus précis mais local)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf_base,\n",
    "    param_grid=param_grid,\n",
    "    cv=3,\n",
    "    scoring='r2',\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Meilleur modèle trouvé après les deux recherches\n",
    "\n",
    "rf = grid_search.best_estimator_\n",
    "best_params_final = grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c0ec29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------------------------------------\n",
    "#  Évaluation des performances sur le hold-out\n",
    "# ------------------------------------------------------------\n",
    "r2_tr = r2_score(y_train, rf.predict(X_train))\n",
    "r2_val = r2_score(y_val, rf.predict(X_val))\n",
    "print(f\"R² (train): {r2_tr:.4f} | R² (val): {r2_val:.4f}\")\n",
    "\n",
    "# Calcul des importances de features via permutation_importance\n",
    "\n",
    "try:\n",
    "    perm = permutation_importance(rf, X_val, y_val, n_repeats=5, random_state=42, n_jobs=-1)\n",
    "    imp = (\n",
    "        pd.DataFrame({\"feature\": X_val.columns, \"perm_importance\": perm.importances_mean})\n",
    "        .sort_values(\"perm_importance\", ascending=False)\n",
    "        .head(10)\n",
    "    )\n",
    "    print(\"\\nTop 10 features importantes :\")\n",
    "    print(imp.to_string(index=False))\n",
    "except Exception:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f749c6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "#  Entraînement final sur tout le jeu d'entraînement + prédictions test\n",
    "# ------------------------------------------------------------\n",
    "rf_final = RandomForestRegressor(**best_params_final, oob_score=False, n_jobs=-1, random_state=42)\n",
    "rf_final.fit(X_full_train, y)\n",
    "preds_test = rf_final.predict(X_full_test)\n",
    "\n",
    "submission = pd.DataFrame({\"row_id\": test_row_id, \"popularity\": preds_test})\n",
    "submission.to_csv(\"submission_random_forest.csv\", index=False, sep=\",\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412dda31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "#  Sauvegarde des hyperparamètres et résultats\n",
    "# ------------------------------------------------------------\n",
    "with open(\"best_hyperparameters.txt\", \"w\") as f:\n",
    "    f.write(\"Optimisation des hyperparamètres Random Forest\\n\")\n",
    "    f.write(\"RandomizedSearchCV:\\n\")\n",
    "    for param, value in random_search.best_params_.items():\n",
    "        f.write(f\"{param}: {value}\\n\")\n",
    "    f.write(f\"Best CV R²: {random_search.best_score_:.4f}\\n\\n\")\n",
    "    f.write(\"GridSearchCV:\\n\")\n",
    "    for param, value in best_params_final.items():\n",
    "        f.write(f\"{param}: {value}\\n\")\n",
    "    f.write(f\"Best CV R²: {grid_search.best_score_:.4f}\\n\")\n",
    "    f.write(f\"Validation R²: {r2_val:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39da969",
   "metadata": {},
   "source": [
    "#   CATBOOST REGRESSOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9308c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Chargement des données ---\n",
    "\n",
    "print(\"48+ Chargement des données...\")\n",
    "train = pd.read_csv(\"train_data_with_genres.csv\", sep=\";\", encoding=\"utf-8\", low_memory=False)\n",
    "test = pd.read_csv(\"test_data_with_genres.csv\", sep=\";\", encoding=\"utf-8\", low_memory=False)\n",
    "\n",
    "# Vérification de la présence de la colonne cible 'popularity'\n",
    "\n",
    "if \"popularity\" not in train.columns:\n",
    "    raise ValueError(\"La colonne 'popularity' est absente du fichier d'entraînement.\")\n",
    "# Définition de la variable cible (y) et des features (X)\n",
    "\n",
    "y_train = train[\"popularity\"]\n",
    "X_train = train.drop(columns=[\"popularity\"], errors=\"ignore\")\n",
    "\n",
    "# Suppression éventuelle de la colonne 'row_id' dans le jeu d'entraînement\n",
    "\n",
    "if \"row_id\" in X_train.columns:\n",
    "    X_train = X_train.drop(columns=[\"row_id\"])\n",
    "# Même nettoyage pour le jeu de test\n",
    "\n",
    "if \"row_id\" in test.columns:\n",
    "    row_ids = test[\"row_id\"]\n",
    "    test = test.drop(columns=[\"row_id\"])\n",
    "else:\n",
    "# Si la colonne n'existe pas, on crée un identifiant par défaut\n",
    "    row_ids = pd.Series(range(len(test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95582f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion de toutes les colonnes en valeurs numériques\n",
    "# Les valeurs non convertibles deviennent NaN\n",
    "X_train = X_train.apply(pd.to_numeric, errors=\"coerce\")\n",
    "test = test.apply(pd.to_numeric, errors=\"coerce\")\n",
    "# Remplacement des valeurs manquantes par la médiane de chaque colonne\n",
    "\n",
    "X_train = X_train.fillna(X_train.median())\n",
    "test = test.fillna(test.median())\n",
    "# Affichage des dimensions du dataset préparé\n",
    "\n",
    "print(f\" Données préparées : {X_train.shape[0]} échantillons, {X_train.shape[1]} features\")\n",
    "# --- Définition de la validation croisée ---\n",
    "\n",
    "N_FOLDS = 5\n",
    "kf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=42)\n",
    "print(\"\\n Recherche des meilleurs hyperparamètres avec Random Search...\")\n",
    "\n",
    "# --- Définition de l'espace de recherche des hyperparamètres ---\n",
    "\n",
    "param_distributions = {\n",
    "    'iterations': randint(400, 10000),\n",
    "    'learning_rate': uniform(0.01, 0.1),  \n",
    "    'depth': randint(6, 11),  \n",
    "    'l2_leaf_reg': uniform(1, 9),  \n",
    "    'bagging_temperature': uniform(0.1, 1), \n",
    "    'border_count': [64, 128, 254]\n",
    "}\n",
    "\n",
    "# --- Définition du modèle de base ---\n",
    "\n",
    "base_model = CatBoostRegressor(\n",
    "    random_seed=42,\n",
    "    boosting_type='Plain',\n",
    "    loss_function='RMSE',\n",
    "    verbose=False\n",
    ")\n",
    "# --- Recherche aléatoire d'hyperparamètres ---\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=base_model,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=30,  \n",
    "    cv=kf,\n",
    "    scoring='r2',\n",
    "    n_jobs=-1,  \n",
    "    random_state=42,\n",
    "    verbose=2\n",
    ")\n",
    "# Entraînement du modèle sur le train avec recherche aléatoire\n",
    "\n",
    "random_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02d3c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Résultats de la recherche ---\n",
    "\n",
    "print(\"\\n Meilleurs hyperparamètres trouvés :\")\n",
    "best_params = random_search.best_params_\n",
    "for param, value in best_params.items():\n",
    "    print(f\"  • {param}: {value}\")\n",
    "print(f\"\\n Meilleur R² moyen en CV : {random_search.best_score_:.4f}\")\n",
    "\n",
    "# Conversion des résultats de la recherche en DataFrame pour analyse\n",
    "results_df = pd.DataFrame(random_search.cv_results_)\n",
    "# Affichage des 5 meilleures combinaisons\n",
    "\n",
    "print(f\"\\n Top 5 configurations :\")\n",
    "top5 = results_df.nlargest(5, 'mean_test_score')[['mean_test_score', 'std_test_score', 'params']]\n",
    "for idx, row in top5.iterrows():\n",
    "    print(f\"  R² = {row['mean_test_score']:.4f} ± {row['std_test_score']:.4f}\")\n",
    "\n",
    "# --- Validation croisée manuelle avec les meilleurs paramètres ---\n",
    "\n",
    "print(\"\\n Entraînement du modèle final avec validation croisée...\")\n",
    "\n",
    "# Mise à jour des paramètres finaux du modèle\n",
    "\n",
    "best_params['random_seed'] = 42\n",
    "best_params['boosting_type'] = 'Plain'\n",
    "best_params['loss_function'] = 'RMSE'\n",
    "best_params['eval_metric'] = 'R2'\n",
    "best_params['verbose'] = 200\n",
    "\n",
    "# Listes pour stocker les scores et modèles\n",
    "\n",
    "cv_r2_scores = []\n",
    "cv_rmse_scores = []\n",
    "models = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80336d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boucle sur les 5 folds\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train)):\n",
    "    print(f\"\\n--- Fold {fold + 1}/{N_FOLDS} ---\")\n",
    "    # Séparation des données train/validation pour le fold courant\n",
    "\n",
    "    X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "    # Création et entraînement du modèle\n",
    "\n",
    "    model = CatBoostRegressor(**best_params)\n",
    "    model.fit(X_tr, y_tr, eval_set=(X_val, y_val), early_stopping_rounds=100)\n",
    "    \n",
    "    # Prédictions sur le set de validation\n",
    "\n",
    "    preds_val = model.predict(X_val)\n",
    "\n",
    "    # Calcul des métriques de performance\n",
    "\n",
    "    r2 = r2_score(y_val, preds_val)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, preds_val))\n",
    "\n",
    "    # Sauvegarde des scores et du modèle\n",
    "\n",
    "    cv_r2_scores.append(r2)\n",
    "    cv_rmse_scores.append(rmse)\n",
    "    models.append(model)\n",
    "    \n",
    "    print(f\"  R² : {r2:.4f} | RMSE : {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8d4215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage des scores moyens sur les folds\n",
    "\n",
    "print(f\"\\n Résultats de la validation croisée ({N_FOLDS} folds) :\")\n",
    "print(f\"  • R² moyen  : {np.mean(cv_r2_scores):.4f} ± {np.std(cv_r2_scores):.4f}\")\n",
    "print(f\"  • RMSE moyen: {np.mean(cv_rmse_scores):.4f} ± {np.std(cv_rmse_scores):.4f}\")\n",
    "# --- Entraînement final du modèle sur toutes les données ---\n",
    "\n",
    "print(\"\\n Entraînement du modèle final sur toutes les données d'entraînement...\")\n",
    "final_model = CatBoostRegressor(**best_params)\n",
    "final_model.fit(X_train, y_train)\n",
    "# Évaluation sur le train complet\n",
    "\n",
    "\n",
    "y_pred_train = final_model.predict(X_train)\n",
    "r2_train = r2_score(y_train, y_pred_train)\n",
    "rmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "print(f\"  R² sur train complet  : {r2_train:.4f}\")\n",
    "print(f\"  RMSE sur train complet: {rmse_train:.4f}\")\n",
    "\n",
    "# Sauvegarde du modèle et des hyperparamètres\n",
    "\n",
    "joblib.dump(final_model, \"catboost_spotify_model_optimized.pkl\")\n",
    "print(\"\\n Modèle sauvegardé sous 'catboost_spotify_model_optimized.pkl'\")\n",
    "\n",
    "joblib.dump(best_params, \"best_hyperparameters.pkl\")\n",
    "print(\" Hyperparamètres sauvegardés sous 'best_hyperparameters.pkl'\")\n",
    "\n",
    "# --- Prédictions sur le jeu de test ---\n",
    "\n",
    "print(\"\\n Prédictions sur le jeu de test...\")\n",
    "# Moyenne des prédictions des 5 modèles de CV (ensemble)\n",
    "\n",
    "test_preds = np.zeros(len(test))\n",
    "for model in models:\n",
    "    test_preds += model.predict(test) / len(models)\n",
    "\n",
    "# Création du fichier de soumission\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"row_id\": row_ids,\n",
    "    \"popularity\": test_preds\n",
    "})\n",
    "submission.to_csv(\"submission_catboost_optimized.csv\", index=False)\n",
    "print(\" Fichier 'submission_catboost_optimized.csv' créé avec succès !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ea03ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Vérification des résultats ---\n",
    "\n",
    "print(\"\\n Aperçu de la soumission :\")\n",
    "print(submission.head(10))\n",
    "# Statistiques descriptives des prédictions\n",
    "\n",
    "print(f\"\\nStatistiques des prédictions :\")\n",
    "print(f\"  • Min  : {test_preds.min():.2f}\")\n",
    "print(f\"  • Max  : {test_preds.max():.2f}\")\n",
    "print(f\"  • Moyen: {test_preds.mean():.2f}\")\n",
    "print(f\"  • Std  : {test_preds.std():.2f}\")\n",
    "# --- Importance des features ---\n",
    "print(\"\\n Top 10 features importantes :\")\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': final_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "print(feature_importance.head(10).to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
